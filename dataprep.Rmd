---
title: "Download e Preparação dos Dados"
author: "Mário Olímpio de Menezes"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    fig_height: 6
    fig_width: 8
    toc: yes
    toc_depth: 2

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,  tidy.opts = list(width.cutoff = 80))
```


```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(magrittr)
library(lubridate)
library(ggpubr)
library(readxl)
library(knitr)
library(kableExtra)
library(stringi)
library(rvest)
library(countrycode)
```




# Download das bases

As bases de dados utilizadas são:

* EU Open Data Portal
* John Hopkins University - CCSSE
* Worldometer
* Kaggle - Raphael Fontes

## EU Open Data Portal

Os dados desta seção vieram do [Europe Open Data Portal](https://data.europa.eu/euodp/en/) -- `https://data.europa.eu/euodp/en/`


*2020-03-26:* Houve uma mudança na nomenclatura dos arquivos; a partir da atualização de *25 de Março de 2020* o arquivo não mais tem o sufixo no formato da data, por exemplo, "...`2020-03-24`.xlsx". Assim, algumas partes do código estão sendo mudadas para refletir esta mudança. Não sei se é uma mudança permanente ou alguém esqueceu de colocar o nome correto do arquivo.


```{r}
# alteracoes na nomenclatura dos arquivos e adicao de mais uma coluna (populacao do pais - base 2018 Banco Mundial)
#covid19URLbase <- "https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide-"
covid19URLbase <- "https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide"
hoje <- format(today(), "%Y-%m-%d")   # vou manter para anexar no nome e saber cada atualização feita.
#hoje <- "2020-03-24"
arqdest <- paste("COVID-19-geographic-distribution-worldwide-",hoje,".xlsx",sep="")
```

<!--
https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide-2020-03-18.xls
https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide.xlsx
-->




```{r }
# para fazer o download da base; acerte os caminhos e execute este chunk

if (!file.exists(paste("~/datasets/ECDC_Europe",arqdest,sep="/")) || !file.exists(paste("./data",arqdest,sep="/"))) {
  curl::curl_download(paste(covid19URLbase,".xlsx",sep=""), paste("~/datasets/ECDC_Europe",arqdest,sep="/"))
  print(paste("Fazendo download do arquivo", arqdest,sep=":"))
  file.copy(paste("~/datasets/ECDC_Europe",arqdest,sep="/"), paste("./data",arqdest, sep="/"))
} else {
  hoje <- format(Sys.time() - 1, "%Y-%m-%d")   # vou manter para anexar no nome e saber cada atualização feita.
  arqdest <- paste("COVID-19-geographic-distribution-worldwide-",hoje,".xlsx",sep="")
}
```


```{r leituradados}
covid19prep <- read_excel(paste("./data",arqdest,sep="/"))
```

A estrutura dos dados é:

```{r str}
str(covid19prep)
```

```{r country}
covid19prep <- rename(covid19prep, Country = `countriesAndTerritories`, DateRep = dateRep, 
                      Cases = cases, Deaths = deaths, Day = day, Month = month) %>% 
  select(Country, DateRep, Cases, Deaths, countryterritoryCode, popData2018)
```

```{r}
str(covid19prep)
```

Como a base do EU Open Data Portal contém o código ISO3C dos países, vou utilizar o pacote `countrycode` para uniformizar os nomes dos países e adicionar os continentes. Vou retirar aqueles que não estiverem nesta base.

```{r}
covid19prep <- covid19prep %>% 
  mutate(Country = if_else(!is.na(countrycode(sourcevar = countryterritoryCode, origin = "iso3c", 
                                              destination = "country.name")), 
                           countrycode(sourcevar = countryterritoryCode, origin = "iso3c", 
                                       destination = "country.name"), Country), 
         Continent = countrycode(sourcevar = countryterritoryCode, origin = "iso3c", 
                                 destination = "continent"))
```

Existem alguns problemas na base de dados, que não vou tratar aqui; simplesmente vou gravar deste modo. Na análise decide-se o que será feito com estes dados.

```{r}
covid19prep %>% filter(is.na(Continent))
```



Agora vou salvar este `data.frame` para ser utilizado nas análises. Vou utilizar um nome que indique sempre o último conjunto de dados disponível para esta fonte: `eeuucovid19_last.csv`

```{r}
write_csv(covid19prep, "./data/eeuucovid19_last.csv")
```

## Base de dados da John Hopkins University

O JHU tem um site de monitoramento em tempo real (talvez o mais atualizado) no endereço `https://coronavirus.jhu.edu/map.html`. Os dados utilizados para o mapa estão em um GitHub  `https://github.com/CSSEGISandData/COVID-19`. Tem dados com atualização diária (parece que no mapa as atualizações são mais constantes; no final do dia são atualizados no GitHub).


Este é o link onde aparece o mapa também: `https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases`


### Fazendo o download dos arquivos diários ao invés destes consolidados (não tem o Recovered na última atualização)

O JHU mantém pelo menos 2 formatos dos dados: diários e consolidados (_time series_). Eu tentei utilizar o formato _time series_ inicialmente, mas mudei para o formato diário, de modo que eu mesmo faço a consolidação. Isso permitiu um pouco mais de controle sobre os dados baixados.

Os dados são mantidos neste repositório GitHub:

`https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/`

O nome dos arquivos é MM-DD-YYYY.csv, começando a partir de 22 de Janeiro de 2020.

```{r}
hj <- today()
ontem <- hj - 1
diainicio <- mdy("01-22-2020")
```

```{r}
dia <- diainicio
while(dia <= hj) {
  ultarq <- paste(format(dia,"%m-%d-%Y"),".csv",sep="")
  CSSEbasepath <- "~/datasets/CSSEGISandData/COVID-19/csse_covid_19_daily_reports/"
  ultarq_fullname <- paste(CSSEbasepath,ultarq,sep="") 
  CSSEURLbase <- "https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_daily_reports/"
  if (!file.exists(ultarq_fullname)) {
    tryCatch(expr = {
    curl::curl_download(paste(CSSEURLbase,ultarq,sep=""),ultarq_fullname)
    print(paste("Fazendo dowload do arquivo",ultarq_fullname,sep=":"))
    }, 
    error = function(e) {
      if(dia == hj) { print(e)} else { stop()}
    }
    )
  }
  dia <- dia + 1
}
```

Depois de feito o download de todos os arquivos, é preciso juntá-los em um único `data.frame`. O processo começa com a leitura de todos os `csv`, colocando-os todos em uma `list`.

```{r}
dia <- diainicio
dias <- list()
# mudança de formato a partir de 22 de março de 2020
datamudanca <- ymd("2020-03-22")
while (dia < datamudanca) {
  d1 <- read_csv(paste(CSSEbasepath,format(dia,"%m-%d-%Y"),".csv",sep=""))
  d1 <- rename(d1, Country = `Country/Region`)
  d1 <- group_by(d1,Country) %>% summarise(ConfCases = sum(Confirmed, na.rm = T),
                                           TotDeaths = sum(Deaths, na.rm = T), 
                                           TotRecov = sum(Recovered, na.rm = T)) %>% 
    mutate(DateRep = dia)
  dias <- append(dias,list(d1))
  dia <- dia + 1
}
# novo formato
while(dia <= ontem) {
  d1 <- read_csv(paste(CSSEbasepath,format(dia,"%m-%d-%Y"),".csv",sep=""))
  d1 <- rename(d1, Country = Country_Region)
  d1 <- group_by(d1,Country) %>% summarise(ConfCases = sum(Confirmed, na.rm = T), 
                                           TotDeaths = sum(Deaths, na.rm = T), 
                                           TotRecov = sum(Recovered, na.rm = T)) %>% 
    mutate(DateRep = dia)
  dias <- append(dias, list(d1))
  dia <- dia + 1
  }
```

Com a lista de todos os arquivos (`data.frame`s) diários, a função `bind_rows` do `dplyr` faz a concatenação. Algumas correções de nomes de países são também realizadas, visando normalizar e acertar grafias.

```{r}
X <- bind_rows(dias) %>% arrange(DateRep,Country)
assign("jhucovid19prep",X)
jhucovid19prep <- select(jhucovid19prep, Country, DateRep, ConfirmedCases = ConfCases, Deaths = TotDeaths, Recovered = TotRecov)
jhucovid19prep$Country <- stri_replace_all_regex(jhucovid19prep$Country,"Mainland China", "China", vectorize_all = TRUE)
jhucovid19prep$Country <- stri_replace_all_regex(jhucovid19prep$Country, "UK", "United Kingdom", vectorize_all = TRUE)
rm(X)
```

Vou atribuir um continente fictício para os países não encontrados pela função `countrycode`: *Outro*

```{r}
jhucovid19prep <- jhucovid19prep %>% mutate(Continent = if_else(is.na(countrycode(sourcevar = Country, origin = "country.name", destination = "continent")), "Outro", countrycode(sourcevar = Country, origin = "country.name", destination = "continent")))
```

Alguns países estão com nomes com grafia diferente na base da JHU. Preciso uniformizar isso. Como exemplo, a Korea do Sul.

```{r}
jhucovid19prep %>% select(Country) %>% filter(grepl("Korea",Country)) %>% unique()
```

O Iran também parece que está com nomes grafados de forma diferentes:

```{r}
jhucovid19prep %>% select(Country) %>% filter(grepl("Iran",Country)) %>% unique()
```


Ou seja, estes nomes diferentes geram buracos nos tratamentos quando fazemos um `group_by` **Country**. Então precisamos uniformizar. Para isso, vou utilizar a função `countrycode` do pacote de mesmo nome. A estratégia é gerar os códigos dos países e depois gerar os nomes dos países de volta. Esta estratégia funciona porque a função é *esperta* e conhece muitas grafias diferentes para os nomes dos países de modo a gerar o código correto. E quando faço a conversão de volta, ela gera o nome padronizado. Fantástico este pacote `countrycode` para trabalhar com bases de dados com nomes de países. Como já detectamos acima, alguns *países* não são países de verdade; para estes vou deixar o nome original, já que o continente respectivo está identificado como **Outro** de modo a manter a base completa, mas identificando estes casos estranhos.

```{r}
jhucovid19prep %<>% mutate(Code = countrycode(sourcevar = Country, origin = "country.name", destination = "iso3c"), Country = countrycode(sourcevar = Code, origin = "iso3c", destination = "country.name", nomatch = Country))
```

Verificando quais países ficaram _sem nome_ nesta uniformização, ou seja, mantive os nomes originais.
```{r}
jhucovid19prep %>% select(Country, Code, Continent) %>% filter(Continent == "Outro") %>% unique() %>%
  kable() %>% kable_styling(full_width = F)
```

Estes casos correspondem a este número de observações e casos confirmados:

```{r}
jhucovid19prep %>% select(Country, ConfirmedCases, Continent) %>% 
  filter(Continent == "Outro") %>%
  group_by(Country) %>%
  summarise(TotCasos = last(ConfirmedCases), N = n()) %>% 
  kable() %>% kable_styling(full_width = F)
```

Total de Casos

```{r}
jhucovid19prep %>% select(Country, ConfirmedCases, Continent) %>% 
  filter(Continent == "Outro") %>%
  group_by(Country) %>%
  summarise(TotCasos = last(ConfirmedCases), N = n()) %>% 
  summarise(Total = sum(TotCasos)) %>% 
  kable() %>% kable_styling(full_width = F)
```

 


```{r}
jhucovid19prep <- jhucovid19prep %>%  group_by(DateRep, Country) %>% 
  mutate(ConfirmedCases = sum(ConfirmedCases), Deaths = sum(Deaths), Recovered = sum(Recovered)) %>% 
  ungroup() %>% ungroup()
```


Salvando o arquivo do dia

```{r}
write_csv(jhucovid19prep,paste("./data/","jhucovid19-",ontem,".csv",sep=""))
```

Salvando como último arquivo para ser lido nos outros notebooks.

```{r}
write_csv(jhucovid19prep, glue::glue("./data/","jhucovid19_last.csv"))
```

## Base de Dados do Kaggle - Coronavirus - Brazil

Estes dados são mantidos pelo Raphael Fontes, com uma atualização constante.

Link: `https://www.kaggle.com/unanimad/corona-virus-brazil`

O link direto para os dados é: `https://www.kaggle.com/unanimad/corona-virus-brazil/download` que fornece um arquivo zip com o CSV.

O download da base de dados é feita com a API do Kaggle; é um pacote do Python. A base vem compactada, então depois de fazer o download, utilizo o pacote `zip` para extraí-la.

```{r}
hoje <- today()
system("cd data;/home/mario/.local/bin/kaggle datasets download --force unanimad/corona-virus-brazil")
zip::unzip("./data/corona-virus-brazil.zip", exdir = "./data")
```

## Base de Dados do Brasil a partir do Projeto Brasil.io
https://data.brasil.io/dataset/covid19/caso_full.csv.gz

```{r}
setwd("/datahome/mario/docs/analytics/covid19")
 tryCatch(expr = {
       curl::curl_download("https://data.brasil.io/dataset/covid19/caso_full.csv.gz", destfile = "./data/caso_full.csv.gz") 
       system("cd data;gunzip -f caso_full.csv.gz;cd ..")
    }, 
    error = function(e) {
      print(e)
    }
    )
```

## Base de Dados População Mundial


```{r eval = FALSE}
# populacao mundial - https://population.un.org/wpp/Download/Standard/CSV/
# filtrei apenas 2019 e 2020, peguei os últimos valores de cada ano por pais (Location)
# mantive valores por sexo e também densidade populacional
# baixei este arquivo em 20200419
populacao <- read_csv("./data/WPP2019_TotalPopulationBySex.csv")
```

Filtrei apenas os anos de 2019 e 2020 (previsão).  Removi regiões estranhas que contém a palavra "dependencies". 

```{r eval = FALSE}
pop1920 <- populacao %>% 
  filter(!grepl("dependencies", Location, ignore.case = T), !grepl("develop",Location, ignore.case = T), !grepl("SIDS", Location, ignore.case = T)) %>% 
  filter(Time == 2019 | Time == 2020, Variant == "Medium") %>%
  group_by(Location, Time) %>% 
  summarise(Year = max(Time), PopMale = last(PopMale), PopFemale = last(PopFemale), PopTotal = last(PopTotal), PopDensity = last(PopDensity), VarID = last(VarID), LocID = last(LocID)) %>% 
  select(-Time) %>% dplyr::ungroup()
```

Adicionei uma coluna "Continent" e removi a observações sem continentes (agrupamentos de países ou regiões).

```{r eval = FALSE}
pop1920 <- pop1920 %>% mutate(Continent = if_else(is.na(countrycode(sourcevar = Location, origin = "country.name", destination = "continent")), "Outro", countrycode(sourcevar = Location, origin = "country.name", destination = "continent"))) %>% mutate(Continent = case_when(Location == "Africa" ~ "Africa", Location == "Asia" ~ "Asia", Location == "Europe" ~"Europe", Location == "Central America" ~ "Central America", Location == "North America" ~ "North America", Location == "South America" ~ "South America", Location == "Oceania"  ~ "Oceania", TRUE ~ Continent)) %>% filter(Continent %in% c("Africa","Asia","Europe","Americas","Oceania","North America","South America","Central America"))
```



```{r eval = FALSE}
pop1920 <- pop1920 %>% mutate(Code = countrycode(sourcevar = Location, origin = "country.name", destination = "iso3c"), Country = countrycode(sourcevar = Code, origin = "iso3c", destination = "country.name", nomatch = Location)) %>% select(Country, Code, Year, PopMale, PopFemale, PopTotal, PopDensity, Continent)
```

Gravei este arquivo limpo e preparado para poder utilizar nas análises.

```{r eval = FALSE}
write_csv(pop1920, "./data/populacao20192020.csv")
```

------------

<div><span><a href="https://momenezes.github.io/covid19/"><img src="./back.png" alt="HOME" width="60" style="vertical-align:middle">  <b>HOME  </b> </a></span><span style="color:blue;font-size:8px; padding: 20%;">Página gerada em: "`r format(Sys.time(), '%c')`"</span></div>
